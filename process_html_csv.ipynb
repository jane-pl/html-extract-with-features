{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from bs4 import NavigableString\n",
    "import lxml.html\n",
    "from lxml import etree\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import os\n",
    "import html2text\n",
    "import csv\n",
    "\n",
    "def parse_file(file):\n",
    "    #f = open(file, \"r+\", encoding=\"utf-8\")\n",
    "    #html = f.read()\n",
    "    #f.close()\n",
    "    tree = lxml.html.parse(file)\n",
    "    etree.strip_elements(tree, 'script') #Remove JavaScript\n",
    "    etree.strip_elements(tree, 'style') #Remove style tags\n",
    "    page = tree.getroot()\n",
    "    body = page.cssselect('body')[0] # Do a check that the list is non-empty\n",
    "    return (page, body)\n",
    "\n",
    "def find_winner_div(page, body):\n",
    "    pattern = re.compile(r\"\\s\\s+\", re.MULTILINE)\n",
    "    b_pattern = re.compile(b\"\\s\\s+\", re.MULTILINE)\n",
    "    title = pattern.sub(\" \", page.find(\".//title\").text)\n",
    "    article = body.find(\".//article\")\n",
    "    if (article is not None):\n",
    "        # First, check for the tag <article>\n",
    "        winner = article\n",
    "    else:\n",
    "        # If that fails, apply div logic.\n",
    "        divs = body.cssselect('div')\n",
    "        max_text_ratio = 0\n",
    "        div_tuples = []\n",
    "        for div in divs:\n",
    "            #Calculate ratio of text to all content for each div\n",
    "            ratio = len(pattern.sub(\"\", div.text_content()))/len(b_pattern.sub(b\"\", etree.tostring(div)))\n",
    "            div_tuples.append((div, ratio))\n",
    "            if ratio > max_text_ratio:\n",
    "                max_ratio_div = div\n",
    "                max_text_ratio = ratio\n",
    "        sorted_divs = sorted(div_tuples, key=lambda div:div[1], reverse=True) #Sort by ratio, highest ratio first\n",
    "        candidate_divs = []\n",
    "        if (len(sorted_divs) > 6):\n",
    "            candidate_divs = [item[0] for item in sorted_divs[0:6]]\n",
    "        else:\n",
    "            candidate_divs = [item[0] for item in sorted_divs]\n",
    "        # Tokenize by words\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        max_words = 0\n",
    "        for div in candidate_divs:\n",
    "            tokens = tokenizer.tokenize(div.text_content())\n",
    "            #print(tokens)\n",
    "            num_tokens = len(tokenizer.tokenize(div.text_content()))\n",
    "            if num_tokens > max_words:\n",
    "                max_words = num_tokens\n",
    "                winner = div\n",
    "    #Remove whitespace from the final text\n",
    "    #winner_text = winner.text_content()\n",
    "    winner_text = (etree.tostring(winner)).decode('utf-8')\n",
    "    h2tconv = html2text.HTML2Text()\n",
    "    h2tconv.ignore_links = True\n",
    "    h2tconv.ignore_images = True\n",
    "    winner_text = h2tconv.handle(winner_text)\n",
    "    winner_text = '\\n'.join([' '.join(line.split()) for line in winner_text.splitlines() if line.strip()])\n",
    "    return (title, winner_text, winner)\n",
    "\n",
    "def tag_text (html, text, cur_tags, ti):\n",
    "    if (isinstance (html, NavigableString)):\n",
    "        ctags = cur_tags[:]\n",
    "        text.append((str(html), ctags))\n",
    "        #print(\"Tags: \")\n",
    "        #print(ctags)\n",
    "        #print(\"Text: %s\" %text)\n",
    "        return (text, cur_tags)\n",
    "    else:\n",
    "        children = html.contents\n",
    "        #print(\"Current tag: %s\" %html.name)\n",
    "        for child in children:\n",
    "            #Add current tag to list\n",
    "            if ((not isinstance (child, NavigableString))\n",
    "                and child.name in ti):\n",
    "                tag = child.name\n",
    "                cur_tags.append(tag)\n",
    "            (text, cur_tags) = tag_text(child, text, cur_tags, ti)\n",
    "        if (len(cur_tags) > 0):\n",
    "            cur_tags.pop()\n",
    "        return (text, cur_tags)\n",
    "\n",
    "def get_sentences(winner_div, tags_to_include):\n",
    "    wtext = ((etree.tostring(winner_div)).decode('utf-8'))\n",
    "    div_soup = BeautifulSoup(wtext, \"lxml\")\n",
    "    #tags_to_include = ['p', 'em', 'i', 'b', 'strong', 'mark', 'small', 'ins', 'u']\n",
    "    (text, cur_tags) = tag_text(div_soup, [], [], tags_to_include)\n",
    "    pattern = re.compile(r'\\<(.*?)\\>')\n",
    "    text = [x for x in text if (not pattern.findall(x[0]) and not x[0].isspace())]\n",
    "    sentences = []\n",
    "    for elem in text:\n",
    "        t = elem[0]\n",
    "        f = elem[1]\n",
    "        sents = tokenizer.tokenize(t)\n",
    "        for s in sents:\n",
    "            sentences.append((s, f))\n",
    "#    print(sentences)\n",
    "    return sentences\n",
    "\n",
    "def write_data(sentences, csv_file):\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as fp:\n",
    "        writer = csv.writer(fp, delimiter=',')\n",
    "        for s in sentences:\n",
    "            row = [s[0]] + s[1]\n",
    "            writer.writerow(row)\n",
    "        #writer.writerows(sentences)    \n",
    "#    f = open(text_t_file, 'w', encoding=\"utf-8\")\n",
    "#    f.write(winner_text)\n",
    "#    f.close()\n",
    "\n",
    "\n",
    "tags_to_include = ['p', 'em', 'i', 'b', 'strong', 'mark', 'small', 'ins', 'u']\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "cur_sub_tdir = \"C:/Users/Filip/jnazaro/letters-data/html/times-union/\"\n",
    "if (os.path.isdir(cur_sub_tdir)):\n",
    "    file_list = sorted(os.listdir(cur_sub_tdir))\n",
    "    for file in file_list:\n",
    "        if (not os.path.isdir(file) and \".html\" in file and \".txt\" not in file):\n",
    "            html_file = os.path.join(cur_sub_tdir, file)\n",
    "            file = file.replace('.html', '.txt')\n",
    "            csv_file = html_file.replace('.html', '.csv')\n",
    "            if (os.path.isfile(html_file)):\n",
    "                (page, body) = parse_file(html_file)\n",
    "                (title, winner_text, winner) = find_winner_div(page, body)\n",
    "                sentences = get_sentences(winner, tags_to_include)\n",
    "                sentences = [(title, [])] + sentences\n",
    "                write_data(sentences, csv_file)\n",
    "\n",
    "#html_file = \"C:/Users/Filip/jnazaro/letters-data/test/ct-chicago-barbara-byrd-bennett-scandal-cps-bribe-20170425-story.html\"\n",
    "#csv_file = \"C:/Users/Filip/jnazaro/letters-data/test/ct-chicago-barbara-byrd-bennett-scandal-cps-bribe-20170425-story.csv\"\n",
    "#(page, body) = parse_file(html_file)\n",
    "#(title, winner_text, winner) = find_winner_div(page, body)\n",
    "#sentences = get_sentences(winner, tags_to_include)\n",
    "#sentences = [(title, [])] + sentences\n",
    "#write_data(sentences, csv_file)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
